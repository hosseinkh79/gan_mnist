import torch
from tqdm.auto import tqdm

import configs

true_label = 1
fake_label = 0

#Train and test 
def one_step_train(net_D, net_G, train_dataloader, loss_fn, optimizer_D, optimizer_G, device):
    net_D = net_D.to(device)
    net_G = net_G.to(device)

    net_D.train()
    net_G.train()
    
    # D_loss for loss dis by summation of true and fake losses in one epoch
    # G_loss loss for gen in one epoch
    D_loss, G_loss = 0, 0  

    # Mean of our predictions for real pics and fake pics in dis and generator
    D_real_mean, D_fake_mean, D_G_mean = 0, 0 ,0

    for batch, (X, _) in tqdm(enumerate(train_dataloader)):

        optimizer_D.zero_grad()
        
        X = X.to(device)
        batch_size = X.shape[0]

        true_labels = torch.full((batch_size, ), true_label, dtype=torch.float)
        true_labels = true_labels.to(device)

        fake_labels = torch.full((batch_size, ), fake_label, dtype=torch.float)
        fake_labels = fake_labels.to(device)
        
        #loss for discriminator for : maximize log(D(x)) + log(1 - D(G(z)))
        #actually minimize : -(log(D(x)) + log(1 - D(G(z))))
        #loss for true pics that we have two parts for log(D(x)) and log(1 - D(G(z)))

        #feed true pics for discriminator
        output = net_D(X).view(-1)
        #save our output mean to track model
        D_real = output.mean().item()
        D_real_mean += D_real
        
        loss_true_dis = loss_fn(output, true_labels)
        loss_true_dis.backward()


        #loss for fake pics generated by generator
        #create latent vector
        noise = torch.randn(batch_size, configs.letent_size_z, 1, 1)
        #create pics with latent vector
        fake_pics = net_G(noise)
        #feed fake pics to discriminator
        output = net_D(fake_pics).view(-1)
        D_fake = output.mean().item()
        D_fake_mean += D_fake

        loss_fake_dis = loss_fn(output, fake_labels)
        loss_fake_dis.backward()

        #This is important part because we calculate gradients for true and fake images in dis but
        #not updated the dis parameters . After we calculate the loss_true_dis and loss_fake_dis 
        #for actuall pics and fake pics respectively, the gradients calculated seperately and  
        #they saved and summed. so the gradients summed and then we want to
        #update the dis parameters.
        optimizer_D.step()

        #Add finall loss of this batch to the loss 
        sum_true_fake_loss = loss_true_dis.item() + loss_fake_dis.item()
        D_loss += sum_true_fake_loss


        #Start Generator loss
        #we want to maximize log(D(G(z)))
        optimizer_G.zero_grad()

        #we use old fake_pics that used in discriminator
        output = net_D(fake_pics).view(-1)
        G_m = output.mean().item()
        D_G_mean += G_m

        #we should use true_label for our g_loss 
        g_loss = loss_fn(output, true_label)
        g_loss.backward()

        g_loss = g_loss.item()
        G_loss += g_loss

        optimizer_G.step()
    
    D_loss = D_loss/len(train_dataloader)
    G_loss = G_loss/len(train_dataloader)

    D_real_mean = D_real_mean/len(train_dataloader)
    D_fake_mean = D_fake_mean/len(train_dataloader)
    D_G_mean = D_G_mean/len(train_dataloader)

    return D_loss, G_loss, D_real_mean, D_fake_mean, D_G_mean


def train(net_D,
          net_G,
          train_dataloader,
          loss_fn,
          optimizer_D,
          optimizer_G,
          device,
          epochs):
    
    results = {
            'D_loss':[],
            'G_loss':[],
            'D_real_mean':[],
            'D_fake_mean':[],
            'D_G_mean':[],
        }
    
    for epoch in range(epochs):

        D_loss, G_loss, D_real_mean, D_fake_mean, D_G_mean = one_step_train(net_D=net_D,
                                                                            net_G=net_G,
                                                                            train_dataloader=train_dataloader,
                                                                            loss_fn=loss_fn,
                                                                            optimizer_D=optimizer_D,
                                                                            optimizer_G=optimizer_G,
                                                                            device=device)

        results['D_loss'].append(D_loss)
        results['G_loss'].append(G_loss)
        results['D_real_mean'].append(D_real_mean)
        results['D_fake_mean'].append(D_fake_mean)
        results['D_G_mean'].append(D_G_mean)

        print(
          f"Epoch: {epoch+1} | "
          f"D_loss: {D_loss:.4f} | "
          f"G_loss: {G_loss:.4f} | "
          f"D_real_mean: {D_real_mean:.4f} | "
          f"D_fake_mean: {D_fake_mean:.4f}"
          f"D_G_mean: {D_G_mean:.4f}"
        )
        
    return results


